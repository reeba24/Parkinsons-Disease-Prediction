# -*- coding: utf-8 -*-
"""main project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yXu42YUn39lwjyBRuPRl2evxeU84Jbs3
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report,confusion_matrix, precision_score, recall_score, auc,roc_curve,accuracy_score,f1_score
from sklearn.metrics import PrecisionRecallDisplay, RocCurveDisplay
from sklearn.metrics import ConfusionMatrixDisplay, classification_report
from termcolor import colored
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier

df=pd.read_csv('parkinsons.csv')
print(df.shape,'\n')
df.head()

df.info()

df.isna().sum() #no missing values

df.duplicated().sum() #no duplicated rows

#df.skew().sort_values(ascending=False)#check for skewness negative means left skewed and positive is right skewed

feature = 'MDVP:Fo(Hz)'
meanData = 'Mean : ' + str(round(df[feature].mean(),4))        # variable to contain mean of the attribute
skewData = 'Skewness : ' + str(round(df[feature].skew(),4))    # variable to contain skewness of the attribute
plt.figure(figsize=(10,5))                                         # setting figure size with width = 10 and height = 5
fig = sns.distplot(df[feature], bins=30, kde=True)             # seaborn distplot to examine distribution of the feature
plt.title("Distribution of feature : "+feature+" having "+meanData+" and "+skewData)   # setting title of the figure
plt.show()

bins = [50,100,150,200,250,300]                                         # defining mdvp_fo_hz bins,
# defining labels of mdvp_fo_hz groups as per bins defined as above
mdvp_fo_hz_group = ['MDVP:Fo(Hz) : 50-100', 'MDVP:Fo(Hz) : 100-150', 'MDVP:Fo(Hz) : 150-200', 'MDVP:Fo(Hz) : 200-250', 'MDVP:Fo(Hz) : 250-300']
pdData_mdvp_fo_hz_bin = pd.cut(df['MDVP:Fo(Hz)'],bins,labels=mdvp_fo_hz_group)  # segmenting data as per bins defined

# putting into pandas crosstab and applying lambda function to take percentage and assigning to mdvp_fo_hz_group_col variable
mdvp_fo_hz_group_col = pd.crosstab(pdData_mdvp_fo_hz_bin,df.status).apply(lambda r: r/r.sum()*100, axis=1)
print(mdvp_fo_hz_group_col)                                                    # printing above crosstab

# plotting a stacked bar chart to show PD status for different mdvp_fo_hz group
mdvp_fo_hz_group_col.div(mdvp_fo_hz_group_col.sum(1).astype(float), axis=0).plot(kind='bar',stacked=True)
plt.title("PD status with different mdvp_fo_hz group")

"""**VISUALIZATION**"""

df["status"].value_counts()

df

def plot_data(df, plot_type, grid_size, fig_size, y = None):
    fig = plt.figure(figsize = fig_size)
    column_names = df.select_dtypes(exclude='object').columns
    for i, column_name in enumerate(column_names):
        fig.add_subplot(grid_size[0], grid_size[1], i + 1)
        if plot_type == 'hist':
            plot = sns.histplot(df[column_name], kde = True, color = 'darkblue')
        elif plot_type == 'boxplot':
             plot = sns.boxplot(y=df[column_name], x=y, color = 'red')
        else:
            raise ValueError("Input value for the parameter 'plot_type' should be 'hist' or 'boxplot'.")
        plot.set_xlabel(column_name, fontsize = 16)
    plt.tight_layout()
plot_data(df, plot_type = 'hist', grid_size = (8,3), fig_size = (10, 12))

"""**TRAIN TEST**"""

X=df.drop(['name','status'],axis=1)
Y=df["status"]

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

minmax = MinMaxScaler()

X_train_scaled = minmax.fit_transform(X_train)
X_test_scaled = minmax.transform(X_test)

"""LOGISTIC REGRESSION"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import cross_val_score

Model = LogisticRegression(penalty='l2', C=1.0, solver='liblinear',max_iter=100, tol=0.0001)

# Assuming you have X_train_scaled, Y_train, X_test_scaled, Y_test defined

Model.fit(X_train_scaled, Y_train)
y_pred_LR = Model.predict(X_test_scaled)

print("Logistic Regression:")
print("-" * 16)

overall_score = cross_val_score(Model, X_train_scaled, Y_train, cv=10)
model_score = np.average(overall_score)

print("\nTraining Accuracy Score:", round(Model.score(X_train_scaled, Y_train) * 100, 2))
print(f"Cross Validation Score: {round(model_score * 100, 2)}")
print('Precision Score is:', round(precision_score(Y_test, y_pred_LR) * 100, 2))
print('Recall Score is:', round(recall_score(Y_test, y_pred_LR) * 100, 2))
print('F1-Score Score is:', round(f1_score(Y_test, y_pred_LR) * 100, 2))

print("Testing Accuracy Score for logistic regression is :", round(accuracy_score(Y_test,y_pred_LR) * 100, 2))

conf_matrix = confusion_matrix(Y_test,y_pred_LR)
sns.heatmap(conf_matrix, annot=True)
plt.title('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

"""SUPPORT VECTOR MACHINE"""

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.model_selection import cross_val_score

# Assuming you have X_train_scaled, Y_train, X_test_scaled, Y_test defined

SVM = SVC(probability=True, kernel='linear')
SVM.fit(X_train_scaled, Y_train)
y_pred_SVM = SVM.predict(X_test_scaled)

print("Support Vector Machine:")
print("-" * 16)

overall_score_SVM = cross_val_score(SVM, X_train_scaled, Y_train, cv=10)
model_score_SVM = np.average(overall_score_SVM)

print("\nTraining Accuracy Score:", round(SVM.score(X_train_scaled, Y_train) * 100, 2))
print(f"Cross Validation Score: {round(model_score_SVM * 100, 2)}")
print('Precision Score is:', round(precision_score(Y_test, y_pred_SVM) * 100, 2))
print('Recall Score is:', round(recall_score(Y_test, y_pred_SVM) * 100, 2))
print('F1-Score Score is:', round(f1_score(Y_test, y_pred_SVM) * 100, 2))

print("Testing Accuracy Score for support vector machine is :", round(accuracy_score(Y_test,y_pred_SVM) * 100, 2))

conf_matrix = confusion_matrix(Y_test,y_pred_SVM)
sns.heatmap(conf_matrix, annot=True)
plt.title('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

"""DECISION TREE"""

from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
from sklearn.model_selection import cross_val_score

# Define the parameter grid
param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [None, 5, 10],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 5, 10],
    'max_features': [None, 0.5, 0.8]
}

# Create the Decision Tree Classifier
DTC = DecisionTreeClassifier()

# Create GridSearchCV object
grid_search = GridSearchCV(DTC, param_grid, cv=10)

# Fit the grid search to the training data
grid_search.fit(X_train_scaled, Y_train)

# Get the best parameters and best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

# Create a new Decision Tree Classifier with the best parameters
DTC_best = DecisionTreeClassifier(**best_params)

# Fit the new classifier on the training data
DTC_best.fit(X_train_scaled, Y_train)

# Predict on the test data
y_pred_DTC_best = DTC_best.predict(X_test_scaled)

print("Best Decision Tree Classifier:")
print("-" * 30)

overall_score_DTC_best = cross_val_score(DTC_best, X_train_scaled, Y_train, cv=10)
model_score_DTC_best = np.average(overall_score_DTC_best)

print("\nTraining Accuracy Score:", round(DTC_best.score(X_train_scaled, Y_train) * 100, 2))
print(f"Cross Validation Score: {round(model_score_DTC_best * 100, 2)}")
print('Precision Score:', round(precision_score(Y_test, y_pred_DTC_best) * 100, 2))
print('Recall Score:', round(recall_score(Y_test, y_pred_DTC_best) * 100, 2))
print('F1-Score:', round(f1_score(Y_test, y_pred_DTC_best) * 100, 2))
print("Testing Accuracy Score:", round(accuracy_score(Y_test, y_pred_DTC_best) * 100, 2))

conf_matrix = confusion_matrix(Y_test,y_pred_DTC_best)
sns.heatmap(conf_matrix, annot=True)
plt.title('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

"""Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.model_selection import cross_val_score


RFC = RandomForestClassifier(n_estimators=150)
RFC.fit(X_train_scaled, Y_train)
y_pred_RFC = RFC.predict(X_test_scaled)

print('Random Forest Classifier')
print("-" * 16)


overall_score_RFC = cross_val_score(RFC, X_train_scaled, Y_train, cv=10)
model_score_RFC = np.average(overall_score_RFC)

print("\nTraining Accuracy Score:", round(RFC.score(X_train_scaled, Y_train) * 100, 2))
print(f"Cross Validation Score: {round(model_score_RFC * 100, 2)}")
print('Precision Score is:', round(precision_score(Y_test, y_pred_RFC) * 100, 2))
print('Recall Score is:', round(recall_score(Y_test, y_pred_RFC) * 100, 2))
print('F1-Score Score is:', round(f1_score(Y_test, y_pred_RFC) * 100, 2))

print("Testing Accuracy Score for random forest is :", round(accuracy_score(Y_test, y_pred_RFC) * 100, 2))

conf_matrix = confusion_matrix(Y_test,y_pred_RFC)
sns.heatmap(conf_matrix, annot=True)
plt.title('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

""" K Neighbors Classifier"""

KNN = KNeighborsClassifier(n_neighbors=3)
KNN.fit(X_train_scaled, Y_train)
y_pred_KNN = KNN.predict(X_test_scaled)
print('-'*80)
print("K Nearest Neighbor :")

overall_score_KNN = cross_val_score(KNN, X_train_scaled, Y_train, cv=10)
model_score_KNN = np.average(overall_score_KNN)

print("\nTraining Accuracy Score:", round(KNN.score(X_train_scaled, Y_train) * 100, 2))
print(f"Cross Validation Score: {round(model_score_KNN * 100, 2)}")
print('Precision Score is:', round(precision_score(Y_test, y_pred_KNN) * 100, 2))
print('Recall Score is:', round(recall_score(Y_test, y_pred_KNN) * 100, 2))
print('F1-Score Score is:', round(f1_score(Y_test, y_pred_KNN) * 100, 2))

print("Testing Accuracy Score for KNN is :", round(accuracy_score(Y_test, y_pred_KNN) * 100, 2))

KNN = KNeighborsClassifier(n_neighbors=5)
KNN.fit(X_train_scaled, Y_train)
y_pred_KNN = KNN.predict(X_test_scaled)
print('-'*80)
print("K Nearest Neighbor :")

overall_score_KNN = cross_val_score(KNN, X_train_scaled, Y_train, cv=10)
model_score_KNN = np.average(overall_score_KNN)

print("\nTraining Accuracy Score:", round(KNN.score(X_train_scaled, Y_train) * 100, 2))
print(f"Cross Validation Score: {round(model_score_KNN * 100, 2)}")
print('Precision Score is:', round(precision_score(Y_test, y_pred_KNN) * 100, 2))
print('Recall Score is:', round(recall_score(Y_test, y_pred_KNN) * 100, 2))
print('F1-Score Score is:', round(f1_score(Y_test, y_pred_KNN) * 100, 2))

print("Testing Accuracy Score for KNN is :", round(accuracy_score(Y_test, y_pred_KNN) * 100, 2))

conf_matrix = confusion_matrix(Y_test,y_pred_KNN)
sns.heatmap(conf_matrix, annot=True)
plt.title('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

from sklearn.ensemble import VotingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Define the base models
dt = DecisionTreeClassifier(criterion='entropy', max_depth=100, min_samples_split=10, min_samples_leaf=5, max_features=0.8)
knn = KNeighborsClassifier(n_neighbors=5)

# Create the voting classifier with Decision Tree and KNN
voting_clf = VotingClassifier(estimators=[('dt', dt), ('knn', knn)], voting='soft')

# Train the voting classifier
voting_clf.fit(X_train_scaled, Y_train)

# Make predictions using the voting classifier
y_pred_voting = voting_clf.predict(X_test_scaled)

# Evaluate the voting classifier
overall_score_voting = cross_val_score(voting_clf, X_train_scaled, Y_train, cv=10)
model_score_voting = np.average(overall_score_voting)

print('Voting Classifier (Decision Tree + KNN)')
print("-" * 35)

print("\nTraining Accuracy Score:", round(voting_clf.score(X_train_scaled, Y_train) * 100, 2))
print(f"Cross Validation Score: {round(model_score_voting * 100, 2)}")
print('Precision Score is:', round(precision_score(Y_test, y_pred_voting) * 100, 2))
print('Recall Score is:', round(recall_score(Y_test, y_pred_voting) * 100, 2))
print('F1-Score Score is:', round(f1_score(Y_test, y_pred_voting) * 100, 2))
print("Testing Accuracy Score for Decision Tree + KNN is :", round(accuracy_score(Y_test, y_pred_voting) * 100, 2))

X = df[['MDVP:Fo(Hz)','MDVP:Fhi(Hz)','MDVP:Flo(Hz)','DFA','spread1','spread2','D2']]
y = df['status']


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
RFC = RandomForestClassifier(n_estimators=150, random_state=42)
RFC.fit(X_train, y_train)


y_pred = RFC.predict(X_test)

import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()


scaler.fit(X_train)

input_data = np.array([245.51,262.09,231.848,0.631653,-7.156076,0.127642,2.392122]).reshape(1, -1)


std_input_data = scaler.transform(input_data)
prediction = RFC.predict(input_data)
print(prediction)

if prediction[0] == 0:
    print("The person does not have Parkinson's disease.")
else:
    print("The person has Parkinson's disease.")

from sklearn.ensemble import VotingClassifier

# Define the base models
knn = KNeighborsClassifier(n_neighbors=5)
rfc = RandomForestClassifier(n_estimators=150)

# Create the voting classifier
voting_clf = VotingClassifier(estimators=[('knn', knn), ('rfc', rfc)], voting='hard')

# Train the voting classifier
voting_clf.fit(X_train_scaled, Y_train)

# Make predictions using the voting classifier
y_pred_voting = voting_clf.predict(X_test_scaled)

# Evaluate the voting classifier
overall_score_voting = cross_val_score(voting_clf, X_train_scaled, Y_train, cv=10)
model_score_voting = np.average(overall_score_voting)

print('Voting Classifier (KNN + RFC)')
print("-" * 27)

print("\nTraining Accuracy Score:", round(voting_clf.score(X_train_scaled, Y_train) * 100, 2))
print(f"Cross Validation Score: {round(model_score_voting * 100, 2)}")
print('Precision Score is:', round(precision_score(Y_test, y_pred_voting) * 100, 2))
print('Recall Score is:', round(recall_score(Y_test, y_pred_voting) * 100, 2))
print('F1-Score Score is:', round(f1_score(Y_test, y_pred_voting) * 100, 2))

print("Testing Accuracy Score for KNN is :", round(accuracy_score(Y_test, y_pred_voting) * 100, 2))

print(X_train)

def predict(input_data):
    input_data = np.array(input_data).reshape(1, -1)


    std_input_data = scaler.transform(input_data)
    prediction = RFC.predict(input_data)
    #print(prediction)

    if prediction[0] == 0:
        prediction="The person does not have Parkinson's disease"
    else:
        prediction="The person has Parkinson's disease."
    return prediction